- 먼저 요즘 자주 쓰이는 AI는 chatGPT나 claude 같은 애들이 있다.
    - 이런 것들을 LLM 이라 하는데, Large Language Model 이라고 대규모 언어 모델이라는 뜻이다.
- 그러면 반대로 소규모도 있겠네? 하면 있다.
    - SLM 이라고 하는데, 라마3가 대표적이다.

## AI 분류

- LLM
    - 대규모 언어 모델로, chatGPT나 gemini, claude 같은 애들이 대표적이다.
    - 데이터 갯수랑 성능의 관계가 선형이라는 연구 결과가 있다. 그렇기 때문에 모델의 크기를 기우는 데에 주력하는 방식이다.
- SLM
    - 소규모 언어 모델이다. 라마3 같은 애들이 있다.
    - 근데 보통 모델을 막 돌리는게 그리 쉬운거는 아니다. 그래서 SLM 의 수요와 연구도 활발하게 이루어지는 중이다. 작은 모델로 그나마 좋은 성능을 내는 방식이다.
    - 데이터 모델의 크기는 조금 작지만, 그 모델에 학습을 많이 시키면 성능이 올라가게 된다. 이곳에 집중하는것.

## Multimodal

- 우리가 텍스트 기반으로 언어 모델을 활용하는게 보통이었는데, 요즘에는 이것 뿐 아니라 여러 미디어를 활용할수도 있다.
    - 비디오나 오디오와 같은 것들

## LLM 도입 및 활용

- 요즘 회사에서 이런 언어모델들을 빠르게 도입한다.
- 그래서 이거를 어떻게 잘 활용할 수 있을지가 중요하다.
- 활용법에는 크게 4가지 정도가 있는데
    - Pre-training
        - 실제로 바닥부터 모델을 만드는 과정
    - Fine-tuning
        - 그 모델을 용도에 맞게 변경하는 과정
    - Retrieval Augmented Generation (RAG)
        - 
    - Prompt Engineering
- 이렇게가 있다.
    - 여기서 Pre-training이나 Fine-tuning 은 실제로 모델을 만드는 과정과 관련된 것이다. 모델을 만드는 과정이라고 이해하면 된다.
    - RAG와 prompt engineering 은 결과를 더 좋게 만드는 방법에 관한 부분이다.

## Rule based to LLM

- Rule based
    - 이거는 그냥 코딩이라고 이해하면 된다.
    - 필요에 따라 계산하거나 데이터를 주고받는 룰을 만드는 것이다.
    - 코딩을 통해 특징을 체크하고 분류하는 느낌으로 난이도가 어렵고 정확도가 그리 높지 않다.
- Model based machine learning
    - 이미지의 특징을 정해준다.
    - 데이터를 먼저 뽑아두고 모델에 학습시킨다.
    - 다만, 강아지의 특성과 같이 정해진 것에 대한 내용은 잘 알 수 있지만, 전공 지식 혹은 어려운 내용을 보게 된다면 결과가 계속 달라지게 된다.
        - 즉, 특징을 뽑는 사람의 실력이 퀄리티나 결과에 직접적인 영향을 미치는 것이다.
- Deep Learning
    - 위의 Model based machine learning과 가장 큰 차이는 사람이 직접 데이터의 특징을 뽑는 부분이 최소화된다는것!!
    - 아예 데이터에서 직접 특성을 뽑을 수 있다.
        - 여기서의 문제는 어떤 특징을 뽑아갔는지 모른다는것(블랙박스임)
    - 흠 그러면 이거 어떻게 뽑을까?
        - 먼저 데이터의 관점은 vector 로 이루어진다.
            - 즉 데이터를 각각 하나의 위치로 만들어서 연산 → 엄청나게 많은 특징을 적은 갯수로 만들어내는 것이다.
                - vector space에서 가중치를 연산한다.
                    - 이게 어떤 뜻이냐면… 고차원에서 데이터를 이루고 있던 픽셀들을 모델에서 뭔가 가중치를 곱해서 어떤 정보를 뽑아내는 것
                        - 28 x 28 = 784 개의 데이터가 있다고 생각해보면 각각에 데이터들은 서로 독립되어 있다.
                            - 그리고 그 각각의 데이터는 784 차원의 한 점이 된다(서로 영향을 줄 수 없기에)
                                - 근데 이거는 너무 높은 차원이라.. 우리가 알 수 없다.
                                    - 이를 가중치를 통해 낮춰나가는것이다.
                                        - 솔직히 이거는 이해가 어렵다..
    - 말하자면 특정 고차원의 데이터가 입력되면, 딥러닝은 이 벡터들의 위치를 뽑고 저차원으로 압축시켜서 특징을 뽑아내는 것이다… 라고 이해할 수 있을 것 같다.
    - 참고로 이 과정 반대로 하는 디코더도 있다. 그래서 특징을 통해 이미지를 뽑아내는 것도 가능하다.

### pre-trained model

- 위의 딥러닝을 보면 결국 어떤 정보가 들어왔을 때에 이 특징을 잘 뽑아서 저장하는 것이다.
- 따라서 이 딥러닝에 필요한 내용들을 많이 가르쳐 준다면 AI가 빠르게 학습한다는 것이다.
- pre-train 이란 이와 같은 데이터를 학습 모델에 많이 학습시켜 주는 과정이라고 보면 된다.

### Fine Tuning

- 위의 pre-trained 를 보면 특정 목적에 따라 학습을 시킨다.
- 예를 들어 감정 분석, 뉴스 생성 이런 것들을 위해서는 Fine Tuning이 필요하다.

### Big model & Zero/few shot

- 위의 pre-trained, Fine Tuning은 따라서 학습을 시키고, 목적에 맞는 동작을 하도록 만드는 것이다.
- 그런데 LLM 이 나오면서 학습 모델의 크기가 커지면서 새로운 패러다임이 등장했다.
- 얘가 많이 알고 데이터가 커지니까 따로 학습시키지 않은 것들도 수행할 수가 있게 된 것이다!!
- 그래서 걍 학습시키지 않은 것도 부탁하면 해준다.
- 이제는 딱히 학습 없이도 프롬프트만으로도 원하는 동작을 할 수 있다(RAG, prompt engineering)
- 그래서 zero shot / few shot 이 뭐냐면
    - zero shot : 예시를 주지 않고 맞추는것.
    - few shot : 예시를 몇 개 주고 맞추게 하는것.

### NLP(Natural Language Processing)

- 그래서 텍스트를 다루는 과정을 한번 알아본다.
- 자연어 처리가 어려운 이유는 딥러닝에서는 픽셀을 통해 데이터를 저장하는데, 텍스트는 생각해보면 벡터화 시키는게 어렵다.
    - 그 연산을 어떻게 해야할지??
- 보면 자연어는 Sequence data로 시간적, 공간적 순서 관계에 의해 context를 갖는다.
    - 예를 들어 I want to have an apple. 이거는 애플 회사가 아니라 사과겠지.

### RNN

- 내가 모델이라고 한다면
- 내가 ??? / 어제 내 물건을 찾으려 여기에 ???
    - 이러면 아마 뒤에가 더 원하는 응답을 찾기 쉬울 것이다.
- 그러니까 단어는 위의 이미지를 벡터화 하는것과 어느 정도는 비슷하지만 훨씬 높은 차원이라는것
- Word Representation
    - 자연어를 처리하는 방법은 굉장히 많았는데, 딥러닝에서는 보통 `One-hot vector` `Word2Vec` 두개를 사용해서 처리한다.
        - 예를 들어 `You say goodbye [   ] I say hello.` 와 같은 문장이 있고, 가운데를 유추하는 경우라고 한다면.
        - One-hot vector
            - 각각 단어들 (`You`, `say`, `goodbye`) 등등의 각 위치와 다른 단어들에 대해 다음에 어떤 것이 올지를 예측하고 가중치를 곱해준다.
            - 그리고 그 예측을 모델을 통해 계산해서 예측값이 얼마가 될지를 계산한다.
            - 이거는 문제가 각 단어들은 다른 모든 단어들의 갯수만큼 가중치 연산을 해주어야 한다는 것.
                - 막 100만자리 문장이다 이러면 모델을 통해 단어를 벡터화 하려면 엄청 힘겨워지겠지.
        - Word2Vec
            - 결국 우리는 원하는 것은 단어가 벡터화되고 이게 학습 가능하면 되는 것이다.
                - 학습 가능하다. 라는 의미는 모델이 직접 가중치를 튜닝할 수 있다는 것.
            - 그래서 아예 Word Enbedding layer 라는걸 튜닝한다.
            - 이름처럼 Vector 형태로 단어를  변환하는것.
            - 두가지 방식이 있는데
                - CBOW(Continuous Bag of Words)
                    - 주변 문맥을 기반으로 중심 단어를 예측
                - Skip-gram 모델
                    - 중심 단어를 기반으로 주변 단어를 예측
            - skip-gram 모델이 많이 쓰인다.
                - 자주 쓰이는 단어와 드문 단어 모두에 대해 학습할 수 있다.
                - 다양한 크기의 데이터셋에서 성능이 괜찮고, 큰 데이터셋에서도 좋다(드문 단어에 대한 학습도 잘되기 때문에)
                - 문맥을 이해하는것도 뛰어나다.
- 그래서 RNN 은 결국 무엇을 할 수 있지?
    - 순서를 다루는 데이터의 처리가 가능
    - 기계번역, 자연어 처리 등에 활용이 가능하다.

### Transformer

- RNN 보다 발전된 모델이라 보면 좋다.
- RNN
    - 순환 신경망
    - 시퀀스 데이터 처리에 특화되어 있다(즉 시간에 의존적이다.)
    - 긴 시퀀스에 대해 학습 효율이 낮을 수 있습니다.
- Transformer
    - 병렬 처리 가능 → 시간에 의존적이지 않다.
    - 언어 번역 등에 특히 유용하다. 속도 또한 장점이 있다.
    - self-attention
        - 입력 시퀀스의 각 단어와 동일 시퀀스 내 단어들의 관련도를 계산.
            - 시퀀스의 단어를 벡터로 만들어 query, key, value 각각을 만들어준다.
                - query : 각 단어가 다른 단어들과 얼마나 관련이 있는가
                - key : 단어의 특성
                - value : 각 단어의 최종 정보
            - query - key 사이의 내적을 계산해서 단어 간 유사도 구한다. 이게 attention score이다.
            - 그렇게 구한 유사도 등을 통해 문맥적으로 중요도를 구해낸다.
        - 앞의 내용 중 중요해 보이는걸 골라서, 이를 통해 다음 단어를 추측할 수 있다.
            - 트랜스포머가 좋기는 한데(사실상 지금 AI에서는 대체가 불가능) 엄청 빡세서 GPU같은게 엄청 많이 필요하다.
            - 얘는 모델 크기에 따라 성능이 선형으로 증가한다. (점점 많은 GPU가 필요함.)
        - 그럼 트랜스포머랑 LLM은 어떤 관계가 있는가?
            - Emergence, scaling law
                - Energence : LLM에서 학습하지 않은 task들을 수행하는 현상
                - 10^22 ~ 10^24 만큼 다른걸 가르쳐놓으면 굳이 학습 없이도 얘가 질문 응답 가능 (이거 어떻게 하는지는 몰러)
            - 모델의 크기와 데이터의 크기가 커지면 성능은 예측 가능하게 증가한다.
                - 근데 한쪽을 고정하면 어느 순간 성능은 향상이 중지된다.
            - 모델의 크기가 커지면 크기에 따른 성능 향상이 보인다.
                - 그래서 요즘 막 점점 모델 크기가 커지는겨
            - 장점
                - 병렬처리 가능 (모든 단어간 관계를 동시에 계산한다. 시퀀스 독립적이다.)
                - 시퀀스가 길어도 되고, 다양한 문맥을 파악할 수 있다.

- Chat GPT 실습
    - Temperature
        - 낮을수록 다양하지 않고, 높을수록 다양한 응답이 나온다. 즉 낮으면 정석적인 답변
    - Chain Of Thought
        - 예시로 풀이 방법을 제안하고, 그렇게 생각하면서 풀게하면 기존보다 정답률이 20% 정도 올라감 …
            
            ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/49a784e3-39b0-4ed1-ab9a-37c0c7b18bbd/de4b5449-1d58-4da4-9b91-2e56344d138c/Untitled.png)
            
        - Zero-shot COT(Chain of Thought)
            
            ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/49a784e3-39b0-4ed1-ab9a-37c0c7b18bbd/ff6b4b96-807c-46b1-b9ae-f543fc50a0ad/Untitled.png)
            
            - 차근차근 해보자~
